{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Transformers Work\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. Overview\n",
    "2. Introduction\n",
    "3. Seq2Seq Models\n",
    "4. Transformer In-depth\n",
    "5. Transformer-XL\n",
    "6. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The Transformer model in NLP has truly changed the way we work with text data. The model is behind the recent NLP developments, including Google's BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Transformers?\n",
    "The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was proposed in the paper \"Attention Is All You Need\". From the paper:\n",
    "\n",
    "> \"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\"\n",
    "\n",
    "By \"transduction\", they mean converting the input sequences into output sequences. The idea behind Transformer is to handle the dependencies between input and output with __attention__ and recurrence completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transformer.py\n",
    "\n",
    "# imports from standard library\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "class Transformer1(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "    ):\n",
    "        super(Transformer1, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = d_model,\n",
    "                                                   nhead = nhead)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model = d_model,\n",
    "                                                   nhead = nhead)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> None:\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements positional encoding as described in \"Attention is All You Need\"\n",
    "    Expects input of size (N, T, E)\n",
    "    Generates positional encoding of size (T, E), and adds this to each batch\n",
    "    element.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features: int, seq_len: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoding for each element is (seq_len x num_features)\n",
    "        positional_encoding = torch.zeros(seq_len, num_features, requires_grad=False)\n",
    "\n",
    "        # Generate position - list from 0 to seq_len\n",
    "        # Reshape to (seq_len x 1)\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "\n",
    "        # These will be divided by\n",
    "        #\n",
    "        # (10000 ^ (i / num_features))\n",
    "        #\n",
    "        # where i is the dim\n",
    "        #\n",
    "        # So, we'll have one feature where the position is divided by 1, giving a\n",
    "        # sine/cosine wave with frequency 2 * pi\n",
    "        #\n",
    "        # At the other extreme, we'll have a feature where the position is divided by\n",
    "        # 10000, giving sine/cosine waves with frequency 2 * pi * 10000\n",
    "        #\n",
    "        # Another way of saying this is that this will be *multiplied* by\n",
    "        # ((1/10000) ^ (i / num_features))\n",
    "        #\n",
    "        # or by\n",
    "        #\n",
    "        # exp ( log (1/10000) ^ (i / num_features) )\n",
    "        #\n",
    "        # or equivalently\n",
    "        #\n",
    "        # exp ( (i / num_features) * -log(10000) )\n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, num_features, 2).float() / num_features)\n",
    "            * math.log(1 / 10000)\n",
    "        )\n",
    "        # Now we alternate applying sine to these features vs. cosine\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a first dimension of size 1\n",
    "        # [seq_len x num_features] -> [1 x seq_len x num_features]\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        # Transpose to put sequence length in first position, batch size in second\n",
    "        positional_encoding = positional_encoding.transpose(0, 1)\n",
    "\n",
    "        # de-mean\n",
    "        # due to all the cosine terms starting at 1, and the sine terms starting at\n",
    "        # 0, the mean of these positional encodings is much greater than 0; adding\n",
    "        # an embedding that is shifted like this seems sub optimal, so we'll\n",
    "        # \"de-mean\" this matrix:\n",
    "        positional_encoding = positional_encoding - positional_encoding.mean()\n",
    "\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.positional_encoding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
